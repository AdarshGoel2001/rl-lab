algorithm:
  lr: 0.00025
  name: ppo
buffer:
  batch_size: 128
  capacity: 128
  type: trajectory
environment:
  max_episode_steps: null
  name: ''
  normalize_obs: false
  normalize_reward: false
  wrapper: atari
evaluation:
  max_episode_steps: null
  name: ''
  normalize_obs: false
  normalize_reward: false
  wrapper: atari
experiment:
  debug: false
  device: auto
  name: ppo_atari_pong
  seed: 42
logging:
  log_frequency: 10000
  tensorboard: true
  terminal: true
  wandb_enabled: false
  wandb_project: null
  wandb_tags: []
network:
  actor:
    activation: relu
    hidden_dims:
    - 64
    - 64
    initialization: xavier_uniform
    input_dim: null
    output_activation: linear
    output_dim: null
    type: nature_cnn
  critic:
    activation: relu
    hidden_dims:
    - 64
    - 64
    initialization: xavier_uniform
    input_dim: null
    output_activation: linear
    output_dim: null
    type: nature_cnn
training:
  checkpoint_frequency: 500000
  eval_frequency: 100000
  num_eval_episodes: 32
  total_timesteps: 10000000
