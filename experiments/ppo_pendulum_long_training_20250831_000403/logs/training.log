2025-08-31 00:04:03,146 - src.core.trainer - INFO - Logging setup complete. Log file: experiments/ppo_pendulum_long_training_20250831_000403/logs/training.log
2025-08-31 00:04:03,147 - src.utils.registry - INFO - Registered algorithm: ppo -> PPOAlgorithm
2025-08-31 00:04:03,148 - src.utils.registry - INFO - Registered algorithm: random -> RandomAgent
2025-08-31 00:04:03,148 - src.utils.registry - INFO - Registered network: mlp -> MLP
2025-08-31 00:04:03,148 - src.utils.registry - INFO - Registered network: actor_mlp -> ActorMLP
2025-08-31 00:04:03,149 - src.utils.registry - INFO - Registered network: critic_mlp -> CriticMLP
2025-08-31 00:04:03,149 - src.utils.registry - INFO - Registered network: continuous_actor_mlp -> ContinuousActorMLP
2025-08-31 00:04:03,149 - src.utils.registry - INFO - Registered network: dueling_mlp -> DuelingMLP
2025-08-31 00:04:03,150 - src.utils.registry - INFO - Registered environment: gym -> GymWrapper
2025-08-31 00:04:03,151 - src.utils.registry - INFO - Registered buffer: trajectory -> TrajectoryBuffer
2025-08-31 00:04:03,151 - src.core.trainer - INFO - Initializing environment...
2025-08-31 00:04:03,196 - src.environments.gym_wrapper - INFO - Created Gym environment: Pendulum-v1
2025-08-31 00:04:03,196 - src.core.trainer - INFO - Environment: Pendulum-v1
2025-08-31 00:04:03,196 - src.core.trainer - INFO - Observation space: (3,)
2025-08-31 00:04:03,196 - src.core.trainer - INFO - Action space: (1,), discrete: False
2025-08-31 00:04:03,196 - src.core.trainer - INFO - Initializing networks...
2025-08-31 00:04:03,221 - src.core.trainer - INFO - Created actor network: continuous_actor_mlp
2025-08-31 00:04:03,222 - src.core.trainer - INFO - Created critic network: critic_mlp
2025-08-31 00:04:03,222 - src.core.trainer - INFO - Initializing algorithm...
2025-08-31 00:04:03,776 - src.core.trainer - INFO - Created algorithm: ppo
2025-08-31 00:04:03,776 - src.core.trainer - INFO - Initializing buffer...
2025-08-31 00:04:03,776 - src.core.trainer - INFO - Created buffer: trajectory
2025-08-31 00:04:03,779 - src.environments.gym_wrapper - INFO - Seed will be applied on next reset: 42
2025-08-31 00:04:03,779 - src.core.trainer - INFO - Random seeds set to 42
2025-08-31 00:04:03,779 - src.core.trainer - INFO - All components initialized successfully
2025-08-31 00:04:03,779 - src.utils.checkpoint - INFO - Checkpoint manager initialized at experiments/ppo_pendulum_long_training_20250831_000403/checkpoints
2025-08-31 00:04:03,780 - src.utils.logger - INFO - TensorBoard logging enabled: experiments/ppo_pendulum_long_training_20250831_000403/tensorboard
2025-08-31 00:04:03,780 - src.utils.logger - INFO - Experiment logger initialized: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:04:03,781 - src.utils.checkpoint - INFO - No checkpoint found to load
2025-08-31 00:04:03,781 - src.core.trainer - INFO - No checkpoint found, starting from scratch
2025-08-31 00:04:03,781 - src.core.trainer - INFO - Trainer initialized for experiment: ppo_pendulum_long_training
2025-08-31 00:04:03,781 - src.core.trainer - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:04:03,783 - src.utils.config - INFO - Configuration saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:04:03,793 - src.core.trainer - INFO - Experiment config saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:04:03,793 - __main__ - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:04:03,793 - __main__ - INFO - Configuration hash: 5eef404d
2025-08-31 00:04:03,793 - __main__ - INFO - Starting training loop...
2025-08-31 00:04:03,793 - src.core.trainer - INFO - Starting training...
2025-08-31 00:04:03,793 - src.core.trainer - INFO - Target steps: 2000000
2025-08-31 00:04:03,793 - src.core.trainer - INFO - Evaluation frequency: 25000
2025-08-31 00:04:03,794 - src.core.trainer - INFO - Checkpoint frequency: 100000
2025-08-31 00:04:32,705 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_100352.pt (step 100352)
2025-08-31 00:04:32,706 - src.core.trainer - INFO - Auto-saved checkpoint at step 100352
2025-08-31 00:04:59,796 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_200704.pt (step 200704)
2025-08-31 00:04:59,797 - src.core.trainer - INFO - Auto-saved checkpoint at step 200704
2025-08-31 00:05:14,476 - src.utils.logger - INFO - Step 256000 | train/policy_loss: 0.3374 | train/value_loss: 115215.5703 | train/entropy_loss: -0.9189 | train/total_loss: 80651.1889 | train/grad_norm: 374.6166 | train/debug/update_mean_avg: 0.1164 | train/debug/update_mean_std: 0.2301 | train/debug/update_mean_min: -0.2356 | train/debug/update_mean_max: 0.3134 | train/debug/update_log_std_avg: -0.3197 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -324.7705 | train/debug/returns_std: 179.9119 | train/debug/returns_min: -722.7244 | train/debug/returns_max: -11.4802 | train/debug/advantages_mean: -0.0260 | train/debug/advantages_std: 1.0369 | train/debug/advantages_min: -1.6710 | train/debug/advantages_max: 2.8171 | train/debug/old_values_mean: -41.6340 | train/debug/old_values_std: 0.1907 | train/debug/value_error_mean: 287.0388 | train/time_elapsed: 70.6828 | train/steps_per_second: 3621.8126
2025-08-31 00:05:14,479 - src.utils.logger - INFO - Step 256000 | train/policy_loss: 0.3374 | train/value_loss: 115215.5703 | train/entropy_loss: -0.9189 | train/total_loss: 80651.1889 | train/grad_norm: 374.6166 | train/debug/update_mean_avg: 0.1164 | train/debug/update_mean_std: 0.2301 | train/debug/update_mean_min: -0.2356 | train/debug/update_mean_max: 0.3134 | train/debug/update_log_std_avg: -0.3197 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -324.7705 | train/debug/returns_std: 179.9119 | train/debug/returns_min: -722.7244 | train/debug/returns_max: -11.4802 | train/debug/advantages_mean: -0.0260 | train/debug/advantages_std: 1.0369 | train/debug/advantages_min: -1.6710 | train/debug/advantages_max: 2.8171 | train/debug/old_values_mean: -41.6340 | train/debug/old_values_std: 0.1907 | train/debug/value_error_mean: 287.0388 | train/time_elapsed: 70.6859 | train/steps_per_second: 3621.6556
2025-08-31 00:16:40,456 - src.core.trainer - INFO - Logging setup complete. Log file: experiments/ppo_pendulum_long_training_20250831_000403/logs/training.log
2025-08-31 00:16:40,459 - src.utils.registry - INFO - Registered algorithm: ppo -> PPOAlgorithm
2025-08-31 00:16:40,459 - src.utils.registry - INFO - Registered algorithm: random -> RandomAgent
2025-08-31 00:16:40,461 - src.utils.registry - INFO - Registered network: mlp -> MLP
2025-08-31 00:16:40,461 - src.utils.registry - INFO - Registered network: actor_mlp -> ActorMLP
2025-08-31 00:16:40,461 - src.utils.registry - INFO - Registered network: critic_mlp -> CriticMLP
2025-08-31 00:16:40,461 - src.utils.registry - INFO - Registered network: continuous_actor_mlp -> ContinuousActorMLP
2025-08-31 00:16:40,461 - src.utils.registry - INFO - Registered network: dueling_mlp -> DuelingMLP
2025-08-31 00:16:40,462 - src.utils.registry - INFO - Registered environment: gym -> GymWrapper
2025-08-31 00:16:40,463 - src.utils.registry - INFO - Registered buffer: trajectory -> TrajectoryBuffer
2025-08-31 00:16:40,463 - src.core.trainer - INFO - Initializing environment...
2025-08-31 00:16:40,506 - src.environments.gym_wrapper - INFO - Created Gym environment: Pendulum-v1
2025-08-31 00:16:40,506 - src.core.trainer - INFO - Environment: Pendulum-v1
2025-08-31 00:16:40,506 - src.core.trainer - INFO - Observation space: (3,)
2025-08-31 00:16:40,506 - src.core.trainer - INFO - Action space: (1,), discrete: False
2025-08-31 00:16:40,506 - src.core.trainer - INFO - Initializing networks...
2025-08-31 00:16:40,547 - src.core.trainer - INFO - Created actor network: continuous_actor_mlp
2025-08-31 00:16:40,548 - src.core.trainer - INFO - Created critic network: critic_mlp
2025-08-31 00:16:40,548 - src.core.trainer - INFO - Initializing algorithm...
2025-08-31 00:16:41,100 - src.core.trainer - INFO - Created algorithm: ppo
2025-08-31 00:16:41,100 - src.core.trainer - INFO - Initializing buffer...
2025-08-31 00:16:41,100 - src.core.trainer - INFO - Created buffer: trajectory
2025-08-31 00:16:41,103 - src.environments.gym_wrapper - INFO - Seed will be applied on next reset: 42
2025-08-31 00:16:41,103 - src.core.trainer - INFO - Random seeds set to 42
2025-08-31 00:16:41,103 - src.core.trainer - INFO - All components initialized successfully
2025-08-31 00:16:41,103 - src.utils.checkpoint - INFO - Checkpoint manager initialized at experiments/ppo_pendulum_long_training_20250831_000403/checkpoints
2025-08-31 00:16:41,104 - src.utils.logger - INFO - TensorBoard logging enabled: experiments/ppo_pendulum_long_training_20250831_000403/tensorboard
2025-08-31 00:16:41,104 - src.utils.logger - INFO - Experiment logger initialized: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:16:41,111 - src.utils.checkpoint - INFO - Checkpoint loaded: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/latest.pt (step 200704)
2025-08-31 00:16:41,111 - src.core.trainer - INFO - Resuming from checkpoint...
2025-08-31 00:16:41,111 - src.utils.checkpoint - INFO - Training state restored to step 200704
2025-08-31 00:16:41,111 - src.core.trainer - INFO - Resumed from step 200704
2025-08-31 00:16:41,111 - src.core.trainer - INFO - Trainer initialized for experiment: ppo_pendulum_long_training
2025-08-31 00:16:41,111 - src.core.trainer - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:16:41,113 - src.utils.config - INFO - Configuration saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:16:41,122 - src.core.trainer - INFO - Experiment config saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:16:41,122 - __main__ - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:16:41,122 - __main__ - INFO - Configuration hash: 5eef404d
2025-08-31 00:16:41,123 - __main__ - INFO - Starting training loop...
2025-08-31 00:16:41,123 - src.core.trainer - INFO - Starting training...
2025-08-31 00:16:41,123 - src.core.trainer - INFO - Target steps: 2000000
2025-08-31 00:16:41,123 - src.core.trainer - INFO - Evaluation frequency: 25000
2025-08-31 00:16:41,123 - src.core.trainer - INFO - Checkpoint frequency: 100000
2025-08-31 00:16:41,804 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_202752.pt (step 202752)
2025-08-31 00:16:41,805 - src.core.trainer - INFO - Auto-saved checkpoint at step 202752
2025-08-31 00:16:55,983 - src.utils.logger - INFO - Step 256000 | train/policy_loss: 0.3470 | train/value_loss: 112599.0934 | train/entropy_loss: -3.4189 | train/total_loss: 78819.5395 | train/grad_norm: 387.3748 | train/time_elapsed: 14.8598 | train/steps_per_second: 17227.6407
2025-08-31 00:16:55,984 - src.utils.logger - INFO - Step 256000 | train/policy_loss: 0.3470 | train/value_loss: 112599.0934 | train/entropy_loss: -3.4189 | train/total_loss: 78819.5395 | train/grad_norm: 387.3748 | train/debug/update_mean_avg: 0.0458 | train/debug/update_mean_std: 0.2423 | train/debug/update_mean_min: -0.2350 | train/debug/update_mean_max: 0.3122 | train/debug/update_log_std_avg: -0.3075 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -282.0484 | train/debug/returns_std: 127.7415 | train/debug/returns_min: -627.3408 | train/debug/returns_max: -0.0310 | train/debug/advantages_mean: 0.2103 | train/debug/advantages_std: 1.0049 | train/debug/advantages_min: -1.9018 | train/debug/advantages_max: 3.3630 | train/debug/old_values_mean: -32.5840 | train/debug/old_values_std: 1.2638 | train/debug/value_error_mean: 252.1938 | train/time_elapsed: 14.8615 | train/steps_per_second: 17225.6920
2025-08-31 00:17:08,212 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_303104.pt (step 303104)
2025-08-31 00:17:08,213 - src.core.trainer - INFO - Auto-saved checkpoint at step 303104
2025-08-31 00:17:34,281 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_403456.pt (step 403456)
2025-08-31 00:17:34,281 - src.core.trainer - INFO - Auto-saved checkpoint at step 403456
2025-08-31 00:18:00,380 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_503808.pt (step 503808)
2025-08-31 00:18:00,381 - src.core.trainer - INFO - Auto-saved checkpoint at step 503808
2025-08-31 00:18:02,502 - src.utils.logger - INFO - Step 512000 | train/policy_loss: 0.3298 | train/value_loss: 113749.2838 | train/entropy_loss: -3.4189 | train/total_loss: 79624.6552 | train/grad_norm: 395.1829 | train/time_elapsed: 81.3797 | train/steps_per_second: 6291.4928
2025-08-31 00:18:02,503 - src.utils.logger - INFO - Step 512000 | train/policy_loss: 0.3298 | train/value_loss: 113749.2838 | train/entropy_loss: -3.4189 | train/total_loss: 79624.6552 | train/grad_norm: 395.1829 | train/debug/update_mean_avg: 0.0458 | train/debug/update_mean_std: 0.2423 | train/debug/update_mean_min: -0.2350 | train/debug/update_mean_max: 0.3122 | train/debug/update_log_std_avg: -0.3075 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -282.0484 | train/debug/returns_std: 127.7415 | train/debug/returns_min: -627.3408 | train/debug/returns_max: -0.0310 | train/debug/advantages_mean: 0.2103 | train/debug/advantages_std: 1.0049 | train/debug/advantages_min: -1.9018 | train/debug/advantages_max: 3.3630 | train/debug/old_values_mean: -32.5840 | train/debug/old_values_std: 1.2638 | train/debug/value_error_mean: 252.1938 | train/time_elapsed: 81.3805 | train/steps_per_second: 6291.4334
2025-08-31 00:18:26,468 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_604160.pt (step 604160)
2025-08-31 00:18:26,470 - src.core.trainer - INFO - Auto-saved checkpoint at step 604160
2025-08-31 00:18:52,937 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_704512.pt (step 704512)
2025-08-31 00:18:52,938 - src.core.trainer - INFO - Auto-saved checkpoint at step 704512
2025-08-31 00:24:20,542 - src.core.trainer - INFO - Logging setup complete. Log file: experiments/ppo_pendulum_long_training_20250831_000403/logs/training.log
2025-08-31 00:24:20,544 - src.utils.registry - INFO - Registered algorithm: ppo -> PPOAlgorithm
2025-08-31 00:24:20,544 - src.utils.registry - INFO - Registered algorithm: random -> RandomAgent
2025-08-31 00:24:20,545 - src.utils.registry - INFO - Registered network: mlp -> MLP
2025-08-31 00:24:20,545 - src.utils.registry - INFO - Registered network: actor_mlp -> ActorMLP
2025-08-31 00:24:20,545 - src.utils.registry - INFO - Registered network: critic_mlp -> CriticMLP
2025-08-31 00:24:20,545 - src.utils.registry - INFO - Registered network: continuous_actor_mlp -> ContinuousActorMLP
2025-08-31 00:24:20,545 - src.utils.registry - INFO - Registered network: dueling_mlp -> DuelingMLP
2025-08-31 00:24:20,547 - src.utils.registry - INFO - Registered environment: gym -> GymWrapper
2025-08-31 00:24:20,548 - src.utils.registry - INFO - Registered buffer: trajectory -> TrajectoryBuffer
2025-08-31 00:24:20,548 - src.core.trainer - INFO - Initializing environment...
2025-08-31 00:24:20,593 - src.environments.gym_wrapper - INFO - Created Gym environment: Pendulum-v1
2025-08-31 00:24:20,593 - src.core.trainer - INFO - Environment: Pendulum-v1
2025-08-31 00:24:20,593 - src.core.trainer - INFO - Observation space: (3,)
2025-08-31 00:24:20,593 - src.core.trainer - INFO - Action space: (1,), discrete: False
2025-08-31 00:24:20,593 - src.core.trainer - INFO - Initializing networks...
2025-08-31 00:24:20,622 - src.core.trainer - INFO - Created actor network: continuous_actor_mlp
2025-08-31 00:24:20,622 - src.core.trainer - INFO - Created critic network: critic_mlp
2025-08-31 00:24:20,622 - src.core.trainer - INFO - Initializing algorithm...
2025-08-31 00:24:21,172 - src.core.trainer - INFO - Created algorithm: ppo
2025-08-31 00:24:21,172 - src.core.trainer - INFO - Initializing buffer...
2025-08-31 00:24:21,172 - src.core.trainer - INFO - Created buffer: trajectory
2025-08-31 00:24:21,174 - src.environments.gym_wrapper - INFO - Seed will be applied on next reset: 42
2025-08-31 00:24:21,174 - src.core.trainer - INFO - Random seeds set to 42
2025-08-31 00:24:21,174 - src.core.trainer - INFO - All components initialized successfully
2025-08-31 00:24:21,175 - src.utils.checkpoint - INFO - Checkpoint manager initialized at experiments/ppo_pendulum_long_training_20250831_000403/checkpoints
2025-08-31 00:24:21,175 - src.utils.logger - INFO - TensorBoard logging enabled: experiments/ppo_pendulum_long_training_20250831_000403/tensorboard
2025-08-31 00:24:21,175 - src.utils.logger - INFO - Experiment logger initialized: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:24:21,184 - src.utils.checkpoint - INFO - Checkpoint loaded: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/latest.pt (step 704512)
2025-08-31 00:24:21,184 - src.core.trainer - INFO - Resuming from checkpoint...
2025-08-31 00:24:21,185 - src.utils.checkpoint - INFO - Training state restored to step 704512
2025-08-31 00:24:21,185 - src.core.trainer - INFO - Resumed from step 704512
2025-08-31 00:24:21,185 - src.core.trainer - INFO - Trainer initialized for experiment: ppo_pendulum_long_training
2025-08-31 00:24:21,185 - src.core.trainer - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:24:21,187 - src.utils.config - INFO - Configuration saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:24:21,200 - src.core.trainer - INFO - Experiment config saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:24:21,200 - __main__ - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:24:21,201 - __main__ - INFO - Configuration hash: 5eef404d
2025-08-31 00:24:21,201 - __main__ - INFO - Starting training loop...
2025-08-31 00:24:21,201 - src.core.trainer - INFO - Starting training...
2025-08-31 00:24:21,201 - src.core.trainer - INFO - Target steps: 2000000
2025-08-31 00:24:21,201 - src.core.trainer - INFO - Evaluation frequency: 25000
2025-08-31 00:24:21,201 - src.core.trainer - INFO - Checkpoint frequency: 100000
2025-08-31 00:24:21,809 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_706560.pt (step 706560)
2025-08-31 00:24:21,810 - src.core.trainer - INFO - Auto-saved checkpoint at step 706560
2025-08-31 00:24:38,261 - src.utils.logger - INFO - Step 768000 | train/policy_loss: 0.3219 | train/value_loss: 79664.8497 | train/entropy_loss: -0.9189 | train/total_loss: 55765.6692 | train/grad_norm: 328.1842 | train/debug/update_mean_avg: 0.1854 | train/debug/update_mean_std: 0.0007 | train/debug/update_mean_min: 0.1835 | train/debug/update_mean_max: 0.1864 | train/debug/update_log_std_avg: 2.3297 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -353.4562 | train/debug/returns_std: 166.5365 | train/debug/returns_min: -597.0441 | train/debug/returns_max: -4.8290 | train/debug/advantages_mean: -0.0428 | train/debug/advantages_std: 1.1054 | train/debug/advantages_min: -1.1064 | train/debug/advantages_max: 4.3280 | train/debug/old_values_mean: -101.1985 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 262.4914 | train/time_elapsed: 17.0598 | train/steps_per_second: 45018.1042
2025-08-31 00:24:38,263 - src.utils.logger - INFO - Step 768000 | train/policy_loss: 0.3219 | train/value_loss: 79664.8497 | train/entropy_loss: -0.9189 | train/total_loss: 55765.6692 | train/grad_norm: 328.1842 | train/debug/update_mean_avg: 0.1854 | train/debug/update_mean_std: 0.0007 | train/debug/update_mean_min: 0.1835 | train/debug/update_mean_max: 0.1864 | train/debug/update_log_std_avg: 2.3297 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -353.4562 | train/debug/returns_std: 166.5365 | train/debug/returns_min: -597.0441 | train/debug/returns_max: -4.8290 | train/debug/advantages_mean: -0.0428 | train/debug/advantages_std: 1.1054 | train/debug/advantages_min: -1.1064 | train/debug/advantages_max: 4.3280 | train/debug/old_values_mean: -101.1985 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 262.4914 | train/time_elapsed: 17.0626 | train/steps_per_second: 45010.8223
2025-08-31 00:24:48,247 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_806912.pt (step 806912)
2025-08-31 00:24:48,248 - src.core.trainer - INFO - Auto-saved checkpoint at step 806912
2025-08-31 00:31:53,209 - src.core.trainer - INFO - Logging setup complete. Log file: experiments/ppo_pendulum_long_training_20250831_000403/logs/training.log
2025-08-31 00:31:53,210 - src.utils.registry - INFO - Registered algorithm: ppo -> PPOAlgorithm
2025-08-31 00:31:53,211 - src.utils.registry - INFO - Registered algorithm: random -> RandomAgent
2025-08-31 00:31:53,211 - src.utils.registry - INFO - Registered network: mlp -> MLP
2025-08-31 00:31:53,212 - src.utils.registry - INFO - Registered network: actor_mlp -> ActorMLP
2025-08-31 00:31:53,212 - src.utils.registry - INFO - Registered network: critic_mlp -> CriticMLP
2025-08-31 00:31:53,212 - src.utils.registry - INFO - Registered network: continuous_actor_mlp -> ContinuousActorMLP
2025-08-31 00:31:53,212 - src.utils.registry - INFO - Registered network: dueling_mlp -> DuelingMLP
2025-08-31 00:31:53,213 - src.utils.registry - INFO - Registered environment: gym -> GymWrapper
2025-08-31 00:31:53,214 - src.utils.registry - INFO - Registered buffer: trajectory -> TrajectoryBuffer
2025-08-31 00:31:53,214 - src.core.trainer - INFO - Initializing environment...
2025-08-31 00:31:53,262 - src.environments.gym_wrapper - INFO - Created Gym environment: Pendulum-v1
2025-08-31 00:31:53,262 - src.core.trainer - INFO - Environment: Pendulum-v1
2025-08-31 00:31:53,262 - src.core.trainer - INFO - Observation space: (3,)
2025-08-31 00:31:53,262 - src.core.trainer - INFO - Action space: (1,), discrete: False
2025-08-31 00:31:53,262 - src.core.trainer - INFO - Initializing networks...
2025-08-31 00:31:53,292 - src.core.trainer - INFO - Created actor network: continuous_actor_mlp
2025-08-31 00:31:53,292 - src.core.trainer - INFO - Created critic network: critic_mlp
2025-08-31 00:31:53,292 - src.core.trainer - INFO - Initializing algorithm...
2025-08-31 00:31:53,845 - src.core.trainer - INFO - Created algorithm: ppo
2025-08-31 00:31:53,845 - src.core.trainer - INFO - Initializing buffer...
2025-08-31 00:31:53,845 - src.core.trainer - INFO - Created buffer: trajectory
2025-08-31 00:31:53,847 - src.environments.gym_wrapper - INFO - Seed will be applied on next reset: 42
2025-08-31 00:31:53,848 - src.core.trainer - INFO - Random seeds set to 42
2025-08-31 00:31:53,848 - src.core.trainer - INFO - All components initialized successfully
2025-08-31 00:31:53,848 - src.utils.checkpoint - INFO - Checkpoint manager initialized at experiments/ppo_pendulum_long_training_20250831_000403/checkpoints
2025-08-31 00:31:53,849 - src.utils.logger - INFO - TensorBoard logging enabled: experiments/ppo_pendulum_long_training_20250831_000403/tensorboard
2025-08-31 00:31:53,849 - src.utils.logger - INFO - Experiment logger initialized: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:31:53,855 - src.utils.checkpoint - INFO - Checkpoint loaded: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/latest.pt (step 806912)
2025-08-31 00:31:53,855 - src.core.trainer - INFO - Resuming from checkpoint...
2025-08-31 00:31:53,856 - src.utils.checkpoint - INFO - Training state restored to step 806912
2025-08-31 00:31:53,856 - src.core.trainer - INFO - Resumed from step 806912
2025-08-31 00:31:53,856 - src.core.trainer - INFO - Trainer initialized for experiment: ppo_pendulum_long_training
2025-08-31 00:31:53,856 - src.core.trainer - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:31:53,858 - src.utils.config - INFO - Configuration saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:31:53,867 - src.core.trainer - INFO - Experiment config saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:31:53,867 - __main__ - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:31:53,867 - __main__ - INFO - Configuration hash: 5eef404d
2025-08-31 00:31:53,868 - __main__ - INFO - Starting training loop...
2025-08-31 00:31:53,868 - src.core.trainer - INFO - Starting training...
2025-08-31 00:31:53,868 - src.core.trainer - INFO - Target steps: 2000000
2025-08-31 00:31:53,868 - src.core.trainer - INFO - Evaluation frequency: 25000
2025-08-31 00:31:53,868 - src.core.trainer - INFO - Checkpoint frequency: 100000
2025-08-31 00:31:54,573 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_808960.pt (step 808960)
2025-08-31 00:31:54,575 - src.core.trainer - INFO - Auto-saved checkpoint at step 808960
2025-08-31 00:32:21,012 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_909312.pt (step 909312)
2025-08-31 00:32:21,013 - src.core.trainer - INFO - Auto-saved checkpoint at step 909312
2025-08-31 00:32:47,610 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1009664.pt (step 1009664)
2025-08-31 00:32:47,611 - src.core.trainer - INFO - Auto-saved checkpoint at step 1009664
2025-08-31 00:32:51,393 - src.utils.logger - INFO - Step 1024000 | train/policy_loss: 0.3324 | train/value_loss: 89749.5623 | train/entropy_loss: -0.9189 | train/total_loss: 62824.9781 | train/grad_norm: 332.2319 | train/debug/update_mean_avg: 0.1855 | train/debug/update_mean_std: 0.0009 | train/debug/update_mean_min: 0.1839 | train/debug/update_mean_max: 0.1867 | train/debug/update_log_std_avg: 2.3298 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -361.8842 | train/debug/returns_std: 176.7315 | train/debug/returns_min: -704.6334 | train/debug/returns_max: -11.6293 | train/debug/advantages_mean: 0.0983 | train/debug/advantages_std: 1.0122 | train/debug/advantages_min: -1.3006 | train/debug/advantages_max: 3.7996 | train/debug/old_values_mean: -130.9165 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 245.5135 | train/time_elapsed: 57.5257 | train/steps_per_second: 17800.7407
2025-08-31 00:32:51,395 - src.utils.logger - INFO - Step 1024000 | train/policy_loss: 0.3324 | train/value_loss: 89749.5623 | train/entropy_loss: -0.9189 | train/total_loss: 62824.9781 | train/grad_norm: 332.2319 | train/debug/update_mean_avg: 0.1855 | train/debug/update_mean_std: 0.0009 | train/debug/update_mean_min: 0.1839 | train/debug/update_mean_max: 0.1867 | train/debug/update_log_std_avg: 2.3298 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -361.8842 | train/debug/returns_std: 176.7315 | train/debug/returns_min: -704.6334 | train/debug/returns_max: -11.6293 | train/debug/advantages_mean: 0.0983 | train/debug/advantages_std: 1.0122 | train/debug/advantages_min: -1.3006 | train/debug/advantages_max: 3.7996 | train/debug/old_values_mean: -130.9165 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 245.5135 | train/time_elapsed: 57.5277 | train/steps_per_second: 17800.1268
2025-08-31 00:35:02,146 - src.core.trainer - INFO - Logging setup complete. Log file: experiments/ppo_pendulum_long_training_20250831_000403/logs/training.log
2025-08-31 00:35:02,148 - src.utils.registry - INFO - Registered algorithm: ppo -> PPOAlgorithm
2025-08-31 00:35:02,148 - src.utils.registry - INFO - Registered algorithm: random -> RandomAgent
2025-08-31 00:35:02,149 - src.utils.registry - INFO - Registered network: mlp -> MLP
2025-08-31 00:35:02,149 - src.utils.registry - INFO - Registered network: actor_mlp -> ActorMLP
2025-08-31 00:35:02,149 - src.utils.registry - INFO - Registered network: critic_mlp -> CriticMLP
2025-08-31 00:35:02,149 - src.utils.registry - INFO - Registered network: continuous_actor_mlp -> ContinuousActorMLP
2025-08-31 00:35:02,149 - src.utils.registry - INFO - Registered network: dueling_mlp -> DuelingMLP
2025-08-31 00:35:02,151 - src.utils.registry - INFO - Registered environment: gym -> GymWrapper
2025-08-31 00:35:02,151 - src.utils.registry - INFO - Registered buffer: trajectory -> TrajectoryBuffer
2025-08-31 00:35:02,152 - src.core.trainer - INFO - Initializing environment...
2025-08-31 00:35:02,198 - src.environments.gym_wrapper - INFO - Created Gym environment: Pendulum-v1
2025-08-31 00:35:02,198 - src.core.trainer - INFO - Environment: Pendulum-v1
2025-08-31 00:35:02,198 - src.core.trainer - INFO - Observation space: (3,)
2025-08-31 00:35:02,198 - src.core.trainer - INFO - Action space: (1,), discrete: False
2025-08-31 00:35:02,198 - src.core.trainer - INFO - Initializing networks...
2025-08-31 00:35:02,234 - src.core.trainer - INFO - Created actor network: continuous_actor_mlp
2025-08-31 00:35:02,234 - src.core.trainer - INFO - Created critic network: critic_mlp
2025-08-31 00:35:02,234 - src.core.trainer - INFO - Initializing algorithm...
2025-08-31 00:35:02,793 - src.core.trainer - INFO - Created algorithm: ppo
2025-08-31 00:35:02,793 - src.core.trainer - INFO - Initializing buffer...
2025-08-31 00:35:02,794 - src.core.trainer - INFO - Created buffer: trajectory
2025-08-31 00:35:02,796 - src.environments.gym_wrapper - INFO - Seed will be applied on next reset: 42
2025-08-31 00:35:02,796 - src.core.trainer - INFO - Random seeds set to 42
2025-08-31 00:35:02,796 - src.core.trainer - INFO - All components initialized successfully
2025-08-31 00:35:02,796 - src.utils.checkpoint - INFO - Checkpoint manager initialized at experiments/ppo_pendulum_long_training_20250831_000403/checkpoints
2025-08-31 00:35:02,797 - src.utils.logger - INFO - TensorBoard logging enabled: experiments/ppo_pendulum_long_training_20250831_000403/tensorboard
2025-08-31 00:35:02,797 - src.utils.logger - INFO - Experiment logger initialized: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:35:02,805 - src.utils.checkpoint - INFO - Checkpoint loaded: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/latest.pt (step 1009664)
2025-08-31 00:35:02,805 - src.core.trainer - INFO - Resuming from checkpoint...
2025-08-31 00:35:02,805 - src.utils.checkpoint - INFO - Training state restored to step 1009664
2025-08-31 00:35:02,806 - src.core.trainer - INFO - Resumed from step 1009664
2025-08-31 00:35:02,806 - src.core.trainer - INFO - Trainer initialized for experiment: ppo_pendulum_long_training
2025-08-31 00:35:02,806 - src.core.trainer - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:35:02,807 - src.utils.config - INFO - Configuration saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:35:02,818 - src.core.trainer - INFO - Experiment config saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:35:02,818 - __main__ - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:35:02,818 - __main__ - INFO - Configuration hash: 5eef404d
2025-08-31 00:35:02,818 - __main__ - INFO - Starting training loop...
2025-08-31 00:35:02,818 - src.core.trainer - INFO - Starting training...
2025-08-31 00:35:02,818 - src.core.trainer - INFO - Target steps: 2000000
2025-08-31 00:35:02,818 - src.core.trainer - INFO - Evaluation frequency: 25000
2025-08-31 00:35:02,818 - src.core.trainer - INFO - Checkpoint frequency: 100000
2025-08-31 00:35:03,430 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1011712.pt (step 1011712)
2025-08-31 00:35:03,431 - src.core.trainer - INFO - Auto-saved checkpoint at step 1011712
2025-08-31 00:35:06,674 - src.utils.logger - INFO - Step 1024000 | train/policy_loss: 0.3229 | train/value_loss: 47308.5584 | train/entropy_loss: -0.9189 | train/total_loss: 33116.2664 | train/grad_norm: 240.6346 | train/debug/update_mean_avg: 0.1846 | train/debug/update_mean_std: 0.0007 | train/debug/update_mean_min: 0.1836 | train/debug/update_mean_max: 0.1864 | train/debug/update_log_std_avg: 2.3281 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -285.4380 | train/debug/returns_std: 133.8863 | train/debug/returns_min: -586.4177 | train/debug/returns_max: -23.5848 | train/debug/advantages_mean: 0.1010 | train/debug/advantages_std: 1.0241 | train/debug/advantages_min: -1.3459 | train/debug/advantages_max: 3.5486 | train/debug/old_values_mean: -130.9165 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 176.5450 | train/time_elapsed: 3.8559 | train/steps_per_second: 265563.7328
2025-08-31 00:35:06,677 - src.utils.logger - INFO - Step 1024000 | train/policy_loss: 0.3229 | train/value_loss: 47308.5584 | train/entropy_loss: -0.9189 | train/total_loss: 33116.2664 | train/grad_norm: 240.6346 | train/debug/update_mean_avg: 0.1846 | train/debug/update_mean_std: 0.0007 | train/debug/update_mean_min: 0.1836 | train/debug/update_mean_max: 0.1864 | train/debug/update_log_std_avg: 2.3281 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -285.4380 | train/debug/returns_std: 133.8863 | train/debug/returns_min: -586.4177 | train/debug/returns_max: -23.5848 | train/debug/advantages_mean: 0.1010 | train/debug/advantages_std: 1.0241 | train/debug/advantages_min: -1.3459 | train/debug/advantages_max: 3.5486 | train/debug/old_values_mean: -130.9165 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 176.5450 | train/time_elapsed: 3.8583 | train/steps_per_second: 265400.1409
2025-08-31 00:35:30,165 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1112064.pt (step 1112064)
2025-08-31 00:35:30,166 - src.core.trainer - INFO - Auto-saved checkpoint at step 1112064
2025-08-31 00:35:57,496 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1212416.pt (step 1212416)
2025-08-31 00:35:57,497 - src.core.trainer - INFO - Auto-saved checkpoint at step 1212416
2025-08-31 00:37:25,013 - src.core.trainer - INFO - Logging setup complete. Log file: experiments/ppo_pendulum_long_training_20250831_000403/logs/training.log
2025-08-31 00:37:25,015 - src.utils.registry - INFO - Registered algorithm: ppo -> PPOAlgorithm
2025-08-31 00:37:25,015 - src.utils.registry - INFO - Registered algorithm: random -> RandomAgent
2025-08-31 00:37:25,016 - src.utils.registry - INFO - Registered network: mlp -> MLP
2025-08-31 00:37:25,016 - src.utils.registry - INFO - Registered network: actor_mlp -> ActorMLP
2025-08-31 00:37:25,016 - src.utils.registry - INFO - Registered network: critic_mlp -> CriticMLP
2025-08-31 00:37:25,016 - src.utils.registry - INFO - Registered network: continuous_actor_mlp -> ContinuousActorMLP
2025-08-31 00:37:25,016 - src.utils.registry - INFO - Registered network: dueling_mlp -> DuelingMLP
2025-08-31 00:37:25,017 - src.utils.registry - INFO - Registered environment: gym -> GymWrapper
2025-08-31 00:37:25,019 - src.utils.registry - INFO - Registered buffer: trajectory -> TrajectoryBuffer
2025-08-31 00:37:25,019 - src.core.trainer - INFO - Initializing environment...
2025-08-31 00:37:25,071 - src.environments.gym_wrapper - INFO - Created Gym environment: Pendulum-v1
2025-08-31 00:37:25,071 - src.core.trainer - INFO - Environment: Pendulum-v1
2025-08-31 00:37:25,071 - src.core.trainer - INFO - Observation space: (3,)
2025-08-31 00:37:25,071 - src.core.trainer - INFO - Action space: (1,), discrete: False
2025-08-31 00:37:25,071 - src.core.trainer - INFO - Initializing networks...
2025-08-31 00:37:25,099 - src.core.trainer - INFO - Created actor network: continuous_actor_mlp
2025-08-31 00:37:25,099 - src.core.trainer - INFO - Created critic network: critic_mlp
2025-08-31 00:37:25,099 - src.core.trainer - INFO - Initializing algorithm...
2025-08-31 00:37:25,657 - src.core.trainer - INFO - Created algorithm: ppo
2025-08-31 00:37:25,657 - src.core.trainer - INFO - Initializing buffer...
2025-08-31 00:37:25,657 - src.core.trainer - INFO - Created buffer: trajectory
2025-08-31 00:37:25,660 - src.environments.gym_wrapper - INFO - Seed will be applied on next reset: 42
2025-08-31 00:37:25,660 - src.core.trainer - INFO - Random seeds set to 42
2025-08-31 00:37:25,660 - src.core.trainer - INFO - All components initialized successfully
2025-08-31 00:37:25,660 - src.utils.checkpoint - INFO - Checkpoint manager initialized at experiments/ppo_pendulum_long_training_20250831_000403/checkpoints
2025-08-31 00:37:25,661 - src.utils.logger - INFO - TensorBoard logging enabled: experiments/ppo_pendulum_long_training_20250831_000403/tensorboard
2025-08-31 00:37:25,661 - src.utils.logger - INFO - Experiment logger initialized: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:37:25,668 - src.utils.checkpoint - INFO - Checkpoint loaded: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/latest.pt (step 1212416)
2025-08-31 00:37:25,668 - src.core.trainer - INFO - Resuming from checkpoint...
2025-08-31 00:37:25,669 - src.utils.checkpoint - INFO - Training state restored to step 1212416
2025-08-31 00:37:25,669 - src.core.trainer - INFO - Resumed from step 1212416
2025-08-31 00:37:25,669 - src.core.trainer - INFO - Trainer initialized for experiment: ppo_pendulum_long_training
2025-08-31 00:37:25,669 - src.core.trainer - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:37:25,671 - src.utils.config - INFO - Configuration saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:37:25,681 - src.core.trainer - INFO - Experiment config saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:37:25,681 - __main__ - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:37:25,681 - __main__ - INFO - Configuration hash: 5eef404d
2025-08-31 00:37:25,681 - __main__ - INFO - Starting training loop...
2025-08-31 00:37:25,681 - src.core.trainer - INFO - Starting training...
2025-08-31 00:37:25,681 - src.core.trainer - INFO - Target steps: 2000000
2025-08-31 00:37:25,681 - src.core.trainer - INFO - Evaluation frequency: 25000
2025-08-31 00:37:25,681 - src.core.trainer - INFO - Checkpoint frequency: 100000
2025-08-31 00:37:26,299 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1214464.pt (step 1214464)
2025-08-31 00:37:26,299 - src.core.trainer - INFO - Auto-saved checkpoint at step 1214464
2025-08-31 00:37:44,246 - src.utils.logger - INFO - Step 1280000 | train/policy_loss: 0.3246 | train/value_loss: 79130.8237 | train/entropy_loss: -0.9189 | train/total_loss: 55391.8537 | train/grad_norm: 285.2292 | train/debug/update_mean_avg: 0.1856 | train/debug/update_mean_std: 0.0009 | train/debug/update_mean_min: 0.1837 | train/debug/update_mean_max: 0.1867 | train/debug/update_log_std_avg: 2.3306 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -370.7769 | train/debug/returns_std: 170.9648 | train/debug/returns_min: -709.4477 | train/debug/returns_max: -43.4847 | train/debug/advantages_mean: -0.0951 | train/debug/advantages_std: 0.8412 | train/debug/advantages_min: -1.2074 | train/debug/advantages_max: 2.9584 | train/debug/old_values_mean: -160.6345 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 226.8083 | train/time_elapsed: 18.5646 | train/steps_per_second: 68948.3243
2025-08-31 00:37:44,249 - src.utils.logger - INFO - Step 1280000 | train/policy_loss: 0.3246 | train/value_loss: 79130.8237 | train/entropy_loss: -0.9189 | train/total_loss: 55391.8537 | train/grad_norm: 285.2292 | train/debug/update_mean_avg: 0.1856 | train/debug/update_mean_std: 0.0009 | train/debug/update_mean_min: 0.1837 | train/debug/update_mean_max: 0.1867 | train/debug/update_log_std_avg: 2.3306 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -370.7769 | train/debug/returns_std: 170.9648 | train/debug/returns_min: -709.4477 | train/debug/returns_max: -43.4847 | train/debug/advantages_mean: -0.0951 | train/debug/advantages_std: 0.8412 | train/debug/advantages_min: -1.2074 | train/debug/advantages_max: 2.9584 | train/debug/old_values_mean: -160.6345 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 226.8083 | train/time_elapsed: 18.5681 | train/steps_per_second: 68935.4766
2025-08-31 00:37:53,922 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1314816.pt (step 1314816)
2025-08-31 00:37:53,923 - src.core.trainer - INFO - Auto-saved checkpoint at step 1314816
2025-08-31 00:38:39,624 - src.core.trainer - INFO - Logging setup complete. Log file: experiments/ppo_pendulum_long_training_20250831_000403/logs/training.log
2025-08-31 00:38:39,627 - src.utils.registry - INFO - Registered algorithm: ppo -> PPOAlgorithm
2025-08-31 00:38:39,627 - src.utils.registry - INFO - Registered algorithm: random -> RandomAgent
2025-08-31 00:38:39,629 - src.utils.registry - INFO - Registered network: mlp -> MLP
2025-08-31 00:38:39,629 - src.utils.registry - INFO - Registered network: actor_mlp -> ActorMLP
2025-08-31 00:38:39,629 - src.utils.registry - INFO - Registered network: critic_mlp -> CriticMLP
2025-08-31 00:38:39,629 - src.utils.registry - INFO - Registered network: continuous_actor_mlp -> ContinuousActorMLP
2025-08-31 00:38:39,629 - src.utils.registry - INFO - Registered network: dueling_mlp -> DuelingMLP
2025-08-31 00:38:39,630 - src.utils.registry - INFO - Registered environment: gym -> GymWrapper
2025-08-31 00:38:39,631 - src.utils.registry - INFO - Registered buffer: trajectory -> TrajectoryBuffer
2025-08-31 00:38:39,631 - src.core.trainer - INFO - Initializing environment...
2025-08-31 00:38:39,682 - src.environments.gym_wrapper - INFO - Created Gym environment: Pendulum-v1
2025-08-31 00:38:39,682 - src.core.trainer - INFO - Environment: Pendulum-v1
2025-08-31 00:38:39,682 - src.core.trainer - INFO - Observation space: (3,)
2025-08-31 00:38:39,682 - src.core.trainer - INFO - Action space: (1,), discrete: False
2025-08-31 00:38:39,682 - src.core.trainer - INFO - Initializing networks...
2025-08-31 00:38:39,714 - src.core.trainer - INFO - Created actor network: continuous_actor_mlp
2025-08-31 00:38:39,715 - src.core.trainer - INFO - Created critic network: critic_mlp
2025-08-31 00:38:39,715 - src.core.trainer - INFO - Initializing algorithm...
2025-08-31 00:38:40,350 - src.core.trainer - INFO - Created algorithm: ppo
2025-08-31 00:38:40,350 - src.core.trainer - INFO - Initializing buffer...
2025-08-31 00:38:40,350 - src.core.trainer - INFO - Created buffer: trajectory
2025-08-31 00:38:40,353 - src.environments.gym_wrapper - INFO - Seed will be applied on next reset: 42
2025-08-31 00:38:40,353 - src.core.trainer - INFO - Random seeds set to 42
2025-08-31 00:38:40,353 - src.core.trainer - INFO - All components initialized successfully
2025-08-31 00:38:40,353 - src.utils.checkpoint - INFO - Checkpoint manager initialized at experiments/ppo_pendulum_long_training_20250831_000403/checkpoints
2025-08-31 00:38:40,354 - src.utils.logger - INFO - TensorBoard logging enabled: experiments/ppo_pendulum_long_training_20250831_000403/tensorboard
2025-08-31 00:38:40,354 - src.utils.logger - INFO - Experiment logger initialized: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:38:40,361 - src.utils.checkpoint - INFO - Checkpoint loaded: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/latest.pt (step 1314816)
2025-08-31 00:38:40,361 - src.core.trainer - INFO - Resuming from checkpoint...
2025-08-31 00:38:40,362 - src.utils.checkpoint - INFO - Training state restored to step 1314816
2025-08-31 00:38:40,362 - src.core.trainer - INFO - Resumed from step 1314816
2025-08-31 00:38:40,362 - src.core.trainer - INFO - Trainer initialized for experiment: ppo_pendulum_long_training
2025-08-31 00:38:40,362 - src.core.trainer - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:38:40,364 - src.utils.config - INFO - Configuration saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:38:40,373 - src.core.trainer - INFO - Experiment config saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:38:40,373 - __main__ - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:38:40,374 - __main__ - INFO - Configuration hash: 5eef404d
2025-08-31 00:38:40,374 - __main__ - INFO - Starting training loop...
2025-08-31 00:38:40,374 - src.core.trainer - INFO - Starting training...
2025-08-31 00:38:40,374 - src.core.trainer - INFO - Target steps: 2000000
2025-08-31 00:38:40,374 - src.core.trainer - INFO - Evaluation frequency: 25000
2025-08-31 00:38:40,374 - src.core.trainer - INFO - Checkpoint frequency: 100000
2025-08-31 00:38:40,981 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1316864.pt (step 1316864)
2025-08-31 00:38:40,982 - src.core.trainer - INFO - Auto-saved checkpoint at step 1316864
2025-08-31 00:39:07,123 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1417216.pt (step 1417216)
2025-08-31 00:39:07,124 - src.core.trainer - INFO - Auto-saved checkpoint at step 1417216
2025-08-31 00:39:35,686 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1517568.pt (step 1517568)
2025-08-31 00:39:35,687 - src.core.trainer - INFO - Auto-saved checkpoint at step 1517568
2025-08-31 00:39:41,384 - src.utils.logger - INFO - Step 1536000 | train/policy_loss: 0.3031 | train/value_loss: 57239.1812 | train/entropy_loss: -0.9189 | train/total_loss: 40067.6827 | train/grad_norm: 226.7461 | train/debug/update_mean_avg: 0.1855 | train/debug/update_mean_std: 0.0007 | train/debug/update_mean_min: 0.1837 | train/debug/update_mean_max: 0.1866 | train/debug/update_log_std_avg: 2.3303 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -381.4879 | train/debug/returns_std: 166.6902 | train/debug/returns_min: -688.2249 | train/debug/returns_max: -11.1375 | train/debug/advantages_mean: -0.1480 | train/debug/advantages_std: 0.7299 | train/debug/advantages_min: -1.2651 | train/debug/advantages_max: 3.4703 | train/debug/old_values_mean: -190.3524 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 211.3528 | train/time_elapsed: 61.0095 | train/steps_per_second: 25176.4070
2025-08-31 00:39:41,387 - src.utils.logger - INFO - Step 1536000 | train/policy_loss: 0.3031 | train/value_loss: 57239.1812 | train/entropy_loss: -0.9189 | train/total_loss: 40067.6827 | train/grad_norm: 226.7461 | train/debug/update_mean_avg: 0.1855 | train/debug/update_mean_std: 0.0007 | train/debug/update_mean_min: 0.1837 | train/debug/update_mean_max: 0.1866 | train/debug/update_log_std_avg: 2.3303 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -381.4879 | train/debug/returns_std: 166.6902 | train/debug/returns_min: -688.2249 | train/debug/returns_max: -11.1375 | train/debug/advantages_mean: -0.1480 | train/debug/advantages_std: 0.7299 | train/debug/advantages_min: -1.2651 | train/debug/advantages_max: 3.4703 | train/debug/old_values_mean: -190.3524 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 211.3528 | train/time_elapsed: 61.0131 | train/steps_per_second: 25174.9367
2025-08-31 00:40:03,378 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1617920.pt (step 1617920)
2025-08-31 00:40:03,380 - src.core.trainer - INFO - Auto-saved checkpoint at step 1617920
2025-08-31 00:40:30,712 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1718272.pt (step 1718272)
2025-08-31 00:40:30,713 - src.core.trainer - INFO - Auto-saved checkpoint at step 1718272
2025-08-31 00:43:43,062 - src.core.trainer - INFO - Logging setup complete. Log file: experiments/ppo_pendulum_long_training_20250831_000403/logs/training.log
2025-08-31 00:43:43,064 - src.utils.registry - INFO - Registered algorithm: ppo -> PPOAlgorithm
2025-08-31 00:43:43,064 - src.utils.registry - INFO - Registered algorithm: random -> RandomAgent
2025-08-31 00:43:43,065 - src.utils.registry - INFO - Registered network: mlp -> MLP
2025-08-31 00:43:43,065 - src.utils.registry - INFO - Registered network: actor_mlp -> ActorMLP
2025-08-31 00:43:43,065 - src.utils.registry - INFO - Registered network: critic_mlp -> CriticMLP
2025-08-31 00:43:43,065 - src.utils.registry - INFO - Registered network: continuous_actor_mlp -> ContinuousActorMLP
2025-08-31 00:43:43,065 - src.utils.registry - INFO - Registered network: dueling_mlp -> DuelingMLP
2025-08-31 00:43:43,067 - src.utils.registry - INFO - Registered environment: gym -> GymWrapper
2025-08-31 00:43:43,068 - src.utils.registry - INFO - Registered buffer: trajectory -> TrajectoryBuffer
2025-08-31 00:43:43,068 - src.core.trainer - INFO - Initializing environment...
2025-08-31 00:43:43,112 - src.environments.gym_wrapper - INFO - Created Gym environment: Pendulum-v1
2025-08-31 00:43:43,112 - src.core.trainer - INFO - Environment: Pendulum-v1
2025-08-31 00:43:43,112 - src.core.trainer - INFO - Observation space: (3,)
2025-08-31 00:43:43,112 - src.core.trainer - INFO - Action space: (1,), discrete: False
2025-08-31 00:43:43,112 - src.core.trainer - INFO - Initializing networks...
2025-08-31 00:43:43,148 - src.core.trainer - INFO - Created actor network: continuous_actor_mlp
2025-08-31 00:43:43,148 - src.core.trainer - INFO - Created critic network: critic_mlp
2025-08-31 00:43:43,148 - src.core.trainer - INFO - Initializing algorithm...
2025-08-31 00:43:43,722 - src.core.trainer - INFO - Created algorithm: ppo
2025-08-31 00:43:43,722 - src.core.trainer - INFO - Initializing buffer...
2025-08-31 00:43:43,722 - src.core.trainer - INFO - Created buffer: trajectory
2025-08-31 00:43:43,724 - src.environments.gym_wrapper - INFO - Seed will be applied on next reset: 42
2025-08-31 00:43:43,724 - src.core.trainer - INFO - Random seeds set to 42
2025-08-31 00:43:43,724 - src.core.trainer - INFO - All components initialized successfully
2025-08-31 00:43:43,725 - src.utils.checkpoint - INFO - Checkpoint manager initialized at experiments/ppo_pendulum_long_training_20250831_000403/checkpoints
2025-08-31 00:43:43,725 - src.utils.logger - INFO - TensorBoard logging enabled: experiments/ppo_pendulum_long_training_20250831_000403/tensorboard
2025-08-31 00:43:43,726 - src.utils.logger - INFO - Experiment logger initialized: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:43:43,733 - src.utils.checkpoint - INFO - Checkpoint loaded: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/latest.pt (step 1718272)
2025-08-31 00:43:43,733 - src.core.trainer - INFO - Resuming from checkpoint...
2025-08-31 00:43:43,734 - src.utils.checkpoint - INFO - Training state restored to step 1718272
2025-08-31 00:43:43,734 - src.core.trainer - INFO - Resumed from step 1718272
2025-08-31 00:43:43,734 - src.core.trainer - INFO - Trainer initialized for experiment: ppo_pendulum_long_training
2025-08-31 00:43:43,734 - src.core.trainer - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:43:43,736 - src.utils.config - INFO - Configuration saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:43:43,748 - src.core.trainer - INFO - Experiment config saved to experiments/ppo_pendulum_long_training_20250831_000403/configs/config.yaml
2025-08-31 00:43:43,748 - __main__ - INFO - Experiment directory: experiments/ppo_pendulum_long_training_20250831_000403
2025-08-31 00:43:43,748 - __main__ - INFO - Configuration hash: 5eef404d
2025-08-31 00:43:43,748 - __main__ - INFO - Starting training loop...
2025-08-31 00:43:43,748 - src.core.trainer - INFO - Starting training...
2025-08-31 00:43:43,748 - src.core.trainer - INFO - Target steps: 2000000
2025-08-31 00:43:43,748 - src.core.trainer - INFO - Evaluation frequency: 25000
2025-08-31 00:43:43,748 - src.core.trainer - INFO - Checkpoint frequency: 100000
2025-08-31 00:43:44,377 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1720320.pt (step 1720320)
2025-08-31 00:43:44,379 - src.core.trainer - INFO - Auto-saved checkpoint at step 1720320
2025-08-31 00:44:03,958 - src.utils.logger - INFO - Step 1792000 | train/policy_loss: 0.3186 | train/value_loss: 43726.5768 | train/entropy_loss: -0.9189 | train/total_loss: 30608.8751 | train/grad_norm: 157.8232 | train/debug/update_mean_avg: 0.1853 | train/debug/update_mean_std: 0.0010 | train/debug/update_mean_min: 0.1837 | train/debug/update_mean_max: 0.1867 | train/debug/update_log_std_avg: 2.3291 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -329.4368 | train/debug/returns_std: 176.3658 | train/debug/returns_min: -718.5312 | train/debug/returns_max: -0.4491 | train/debug/advantages_mean: 0.1585 | train/debug/advantages_std: 1.3251 | train/debug/advantages_min: -1.1618 | train/debug/advantages_max: 4.1427 | train/debug/old_values_mean: -220.0704 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 174.9326 | train/time_elapsed: 20.2090 | train/steps_per_second: 88673.2279
2025-08-31 00:44:03,961 - src.utils.logger - INFO - Step 1792000 | train/policy_loss: 0.3186 | train/value_loss: 43726.5768 | train/entropy_loss: -0.9189 | train/total_loss: 30608.8751 | train/grad_norm: 157.8232 | train/debug/update_mean_avg: 0.1853 | train/debug/update_mean_std: 0.0010 | train/debug/update_mean_min: 0.1837 | train/debug/update_mean_max: 0.1867 | train/debug/update_log_std_avg: 2.3291 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -329.4368 | train/debug/returns_std: 176.3658 | train/debug/returns_min: -718.5312 | train/debug/returns_max: -0.4491 | train/debug/advantages_mean: 0.1585 | train/debug/advantages_std: 1.3251 | train/debug/advantages_min: -1.1618 | train/debug/advantages_max: 4.1427 | train/debug/old_values_mean: -220.0704 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 174.9326 | train/time_elapsed: 20.2123 | train/steps_per_second: 88659.0310
2025-08-31 00:44:11,679 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1820672.pt (step 1820672)
2025-08-31 00:44:11,681 - src.core.trainer - INFO - Auto-saved checkpoint at step 1820672
2025-08-31 00:44:39,694 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/auto_step_1921024.pt (step 1921024)
2025-08-31 00:44:39,696 - src.core.trainer - INFO - Auto-saved checkpoint at step 1921024
2025-08-31 00:45:01,494 - src.core.trainer - INFO - Running evaluation at step 2000000
2025-08-31 00:45:01,793 - src.core.trainer - INFO - Evaluation complete - Mean return: -1246.80
2025-08-31 00:45:01,794 - src.utils.logger - INFO - Step 2000000 | train/policy_loss: 0.3183 | train/value_loss: 53334.1936 | train/entropy_loss: -0.9189 | train/total_loss: 37334.2064 | train/grad_norm: 174.1420 | train/debug/update_mean_avg: 0.1855 | train/debug/update_mean_std: 0.0010 | train/debug/update_mean_min: 0.1836 | train/debug/update_mean_max: 0.1867 | train/debug/update_log_std_avg: 2.3298 | train/debug/update_std_avg: 0.6065 | train/debug/update_std_min: 0.6065 | train/debug/update_std_max: 0.6065 | train/debug/update_action_clipped_pct: 0.0000 | train/debug/update_network_output_shape: 2.0000 | train/debug/returns_mean: -309.5589 | train/debug/returns_std: 171.7669 | train/debug/returns_min: -651.1274 | train/debug/returns_max: -5.8336 | train/debug/advantages_mean: 0.2774 | train/debug/advantages_std: 1.1099 | train/debug/advantages_min: -0.7846 | train/debug/advantages_max: 3.6184 | train/debug/old_values_mean: -244.0825 | train/debug/old_values_std: 0.0000 | train/debug/value_error_mean: 157.8321 | train/time_elapsed: 78.0451 | train/steps_per_second: 25626.2174
2025-08-31 00:45:01,796 - src.utils.logger - INFO - Step 2000000 | eval/eval_step: 2000000.0000 | eval/eval_episode_count: 10.0000 | eval/eval_return_mean: -1246.7966 | eval/eval_return_std: 244.9723 | eval/eval_return_min: -1686.3710 | eval/eval_return_max: -968.5547 | eval/eval_length_mean: 200.0000
2025-08-31 00:45:01,797 - src.core.trainer - INFO - Training completed, running final evaluation...
2025-08-31 00:45:01,797 - src.core.trainer - INFO - Running evaluation at step 2000000
2025-08-31 00:45:02,014 - src.core.trainer - INFO - Evaluation complete - Mean return: -1246.80
2025-08-31 00:45:02,018 - src.utils.checkpoint - INFO - Checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/final.pt (step 2000000)
2025-08-31 00:45:02,020 - src.core.trainer - INFO - Final checkpoint saved: experiments/ppo_pendulum_long_training_20250831_000403/checkpoints/final.pt
2025-08-31 00:45:02,020 - src.core.trainer - INFO - Training completed successfully in 78.3s
2025-08-31 00:45:02,026 - src.utils.logger - INFO - TensorBoard writer closed
2025-08-31 00:45:02,026 - __main__ - INFO - Training completed successfully!
2025-08-31 00:45:02,026 - __main__ - INFO - Final results:
2025-08-31 00:45:02,027 - __main__ - INFO -   final_step: 2000000.0000
2025-08-31 00:45:02,027 - __main__ - INFO -   training_time: 78.2712
2025-08-31 00:45:02,027 - __main__ - INFO -   steps_per_second: 25626.2174
2025-08-31 00:45:02,027 - __main__ - INFO -   policy_loss: 0.3183
2025-08-31 00:45:02,027 - __main__ - INFO -   value_loss: 53334.1936
2025-08-31 00:45:02,027 - __main__ - INFO -   entropy_loss: -0.9189
2025-08-31 00:45:02,027 - __main__ - INFO -   total_loss: 37334.2064
2025-08-31 00:45:02,027 - __main__ - INFO -   grad_norm: 174.1420
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_mean_avg: 0.1855
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_mean_std: 0.0010
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_mean_min: 0.1836
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_mean_max: 0.1867
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_log_std_avg: 2.3298
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_std_avg: 0.6065
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_std_min: 0.6065
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_std_max: 0.6065
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_action_clipped_pct: 0.0000
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/update_network_output_shape: 2.0000
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/returns_mean: -309.5589
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/returns_std: 171.7669
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/returns_min: -651.1274
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/returns_max: -5.8336
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/advantages_mean: 0.2774
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/advantages_std: 1.1099
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/advantages_min: -0.7846
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/advantages_max: 3.6184
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/old_values_mean: -244.0825
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/old_values_std: 0.0000
2025-08-31 00:45:02,027 - __main__ - INFO -   debug/value_error_mean: 157.8321
2025-08-31 00:45:02,027 - __main__ - INFO -   time_elapsed: 78.0451
2025-08-31 00:45:02,027 - __main__ - INFO -   eval_step: 2000000.0000
2025-08-31 00:45:02,027 - __main__ - INFO -   eval_episode_count: 10.0000
2025-08-31 00:45:02,027 - __main__ - INFO -   eval_return_mean: -1246.7966
2025-08-31 00:45:02,027 - __main__ - INFO -   eval_return_std: 244.9723
2025-08-31 00:45:02,027 - __main__ - INFO -   eval_return_min: -1686.3710
2025-08-31 00:45:02,027 - __main__ - INFO -   eval_return_max: -968.5547
2025-08-31 00:45:02,027 - __main__ - INFO -   eval_length_mean: 200.0000
2025-08-31 00:45:02,027 - src.core.trainer - INFO - Trainer cleanup completed
