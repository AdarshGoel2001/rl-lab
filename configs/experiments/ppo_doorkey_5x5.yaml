# PPO MiniGrid DoorKey-5x5 Experiment Configuration
#
# This configuration trains PPO on the MiniGrid DoorKey-5x5 environment.
# The goal is to achieve ≥95% success rate by learning to:
# 1. Pick up the key (yellow key object) 
# 2. Navigate to the door (unlock it with the key)
# 3. Reach the green goal square
#
# EXPECTED RESULTS:
# - Success rate should start near 0% (random policy)
# - Should achieve >50% success within 100k steps  
# - Target: ≥95% success rate within 300k steps
# - Episode length should decrease as agent learns optimal paths

experiment:
  name: "ppo_minigrid_doorkey_5x5"
  seed: 42
  device: "cpu"                # Use CPU unless you have GPU acceleration
  debug: true

algorithm:
  name: "ppo"
  
  # PPO hyperparameters optimized for MiniGrid exploration
  lr: 3e-4
  clip_ratio: 0.2
  value_coef: 0.5
  entropy_coef: 0.05           # Higher entropy for exploration
  
  # Training schedule  
  ppo_epochs: 4
  minibatch_size: 128          # Larger minibatch for CNN training
  max_grad_norm: 0.5
  
  # Advantage computation
  normalize_advantages: true
  clip_value_loss: true

environment:
  name: "MiniGrid-DoorKey-5x5-v0"
  wrapper: "minigrid"          # Use our MiniGrid wrapper
  obs_mode: "image"            # Use CNN-friendly image observations
  normalize_obs: false         # MiniGrid obs already in [0,255] range
  normalize_reward: false      # MiniGrid uses sparse binary rewards
  max_episode_steps: 50        # Limit episode length to prevent wandering
  render_mode: null            # Set to "human" for visualization

network:
  # Actor network (policy)
  actor:
    type: "minigrid_cnn"       # Use our custom MiniGrid CNN
    activation: "relu"         # ReLU activation for CNN
    use_direction: true        # Include agent direction in features
    channels: [16, 32, 64]     # CNN channel progression
    output_activation: "linear" # Linear output for policy logits
    
  # Critic network (value function)
  critic:
    type: "minigrid_cnn"       # Same architecture as actor
    activation: "relu"
    use_direction: true
    channels: [16, 32, 64]
    output_activation: "linear" # Linear output for value estimate

buffer:
  type: "trajectory"
  capacity: 8192              # Larger buffer for more exploration
  batch_size: 8192            # Use full buffer for training
  
  # GAE parameters
  gamma: 0.99                 # Discount factor
  gae_lambda: 0.95            # GAE lambda
  compute_returns: true
  normalize_advantages: true

training:
  total_timesteps: 300000     # 300k steps target
  eval_frequency: 10000       # Evaluate every 10k steps
  checkpoint_frequency: 25000 # Save every 25k steps
  num_eval_episodes: 50       # More eval episodes for success rate stats
  
  # Success-based early stopping
  early_stopping: true
  target_success_rate: 0.95   # Stop when success rate ≥ 95%
  patience: 3                 # Number of evaluations to wait

logging:
  terminal: true
  tensorboard: true
  log_frequency: 2000
  
  # Track MiniGrid-specific metrics
  custom_metrics:
    - "env/success_rate"      # Track success percentage
    - "env/episode_length"    # Track path efficiency
    - "policy/entropy"        # Track exploration
  
  wandb_enabled: true
  wandb:
    project: "rl-lab-minigrid"
    tags: ["ppo", "doorkey", "5x5", "cnn"]
    notes: "PPO learning to solve DoorKey-5x5 with CNN vision"

# Expected performance milestones
expected_performance:
  random_success_rate: 0.01   # Random policy: ~1% success
  learning_threshold: 0.10    # Should exceed 10% within 50k steps
  intermediate_goal: 0.50     # 50% success by 150k steps
  target_success_rate: 0.95   # 95% success by 300k steps
  
  episode_length:
    random_policy: 45         # Random policy hits timeout
    optimal_policy: 15        # Optimal path is ~15 steps
    target_efficiency: 20     # Target average ≤20 steps

# Hyperparameter variants for experimentation
experimentation:
  entropy_coefficients: [0.01, 0.05, 0.1]    # Exploration levels
  buffer_sizes: [4096, 8192, 16384]          # Data collection amounts
  learning_rates: [1e-4, 3e-4, 1e-3]        # Learning speed
  episode_limits: [30, 50, 75]               # Time pressure variants
  
  # CNN architecture variants
  cnn_channels:
    - [8, 16, 32]             # Smaller CNN
    - [16, 32, 64]            # Standard CNN (default)
    - [32, 64, 128]           # Larger CNN

# Troubleshooting guide
troubleshooting:
  no_exploration:
    - "Increase entropy_coef to 0.1 for more random actions"
    - "Check if agent is getting stuck in corners"
    - "Verify action space covers all 7 MiniGrid actions"
    
  slow_learning:
    - "Increase buffer_size to 16384 for more diverse experience"
    - "Try higher learning_rate (1e-3)"
    - "Check success_rate isn't stuck at 0%"
    
  unstable_training:
    - "Lower learning_rate to 1e-4"
    - "Reduce clip_ratio to 0.1"
    - "Check for NaN values in CNN gradients"
    
  success_rate_plateau:
    - "Agent may be learning suboptimal policy"
    - "Try curriculum learning: train longer on simpler task first"
    - "Increase max_episode_steps temporarily to allow exploration"