# PPO CartPole Experiment Configuration
#
# This is your first complete PPO experiment! This config combines:
# - PPO algorithm with tuned hyperparameters
# - CartPole-v1 environment 
# - Appropriate network architecture
# - Training schedule and evaluation settings
#
# HOMEWORK INSTRUCTIONS:
# Once you implement the PPO algorithm, run this experiment with:
# python scripts/train.py --config configs/experiments/ppo_cartpole.yaml
#
# Expected results:
# - Should start learning within 5-10k timesteps  
# - Should solve CartPole (avg reward 195+) within 20-50k timesteps
# - Final performance should be consistently 200+ reward

experiment:
  name: "ppo_cartpole_homework"
  seed: 42                    # For reproducibility
  device: "cpu"               # Use CPU (fine for CartPole)
  debug: true                # Set to true for verbose logging
  paradigm: "model_free"     # Use model-free paradigm trainer

algorithm:
  name: "paradigm_ppo"        # Use paradigm-based PPO implementation
  
  # Core PPO hyperparameters (tuned for CartPole)
  lr: 3e-4
  clip_ratio: 0.2
  value_coef: 0.5
  entropy_coef: 0.01
  
  # Training schedule
  ppo_epochs: 4               # Multiple epochs per batch
  minibatch_size: 64          # Batch size for gradient updates
  max_grad_norm: 0.5          # Gradient clipping
  
  # Advantage computation
  normalize_advantages: true   # Helps training stability
  clip_value_loss: true       # Clips value function updates

environment:
  name: "CartPole-v1"         # Classic control problem
  wrapper: "gym"              # Use standard Gym wrapper
  normalize_obs: false        # CartPole observations don't need normalization
  normalize_reward: false     # CartPole rewards don't need normalization
  max_episode_steps: 500      # Default CartPole episode length

network:
  # Actor network (policy)
  actor:
    type: "actor_mlp"         # Use specialized actor MLP
    hidden_dims: [64, 64]     # Two hidden layers, 64 neurons each
    activation: "tanh"        # Tanh activation (common for policies)
    output_activation: "linear" # Linear output (will apply softmax in algorithm)
    
  # Critic network (value function)  
  critic:
    type: "critic_mlp"        # Use specialized critic MLP
    hidden_dims: [64, 64]     # Same architecture as actor
    activation: "tanh"        # Tanh activation
    output_activation: "linear" # Linear output (value can be any real number)

buffer:
  type: "trajectory"          # On-policy trajectory buffer
  capacity: 2048              # Collect 2048 steps before training
  batch_size: 2048            # Use all collected data for training
  
  # GAE parameters for advantage computation
  gamma: 0.99                 # Discount factor
  gae_lambda: 0.95            # GAE lambda parameter
  compute_returns: true       # Automatically compute returns and advantages
  normalize_advantages: true   # Normalize advantages in buffer

training:
  total_timesteps: 100000     # Total training steps
  eval_frequency: 5000        # Evaluate every 5k steps
  checkpoint_frequency: 10000 # Save checkpoint every 10k steps
  num_eval_episodes: 10       # Number of episodes for evaluation
  
  # Early stopping (optional)
  early_stopping: true
  target_reward: 195          # Stop training if average reward >= 195 
  patience: 3                 # Number of evaluations to wait

logging:
  terminal: true              # Print progress to terminal
  tensorboard: true           # Log to TensorBoard
  log_frequency: 1000         # Log metrics every 1k steps
  
  # Weights & Biases logging (optional)
  wandb_enabled: true        # Set to true if you want to use W&B
  wandb:
    project: "rl-lab-homework"
    tags: ["ppo", "cartpole", "homework"]
    notes: "First PPO implementation on CartPole"

# Hyperparameter suggestions for experimentation
# Once your basic implementation works, try these variations:
experimentation:
  learning_rates: [1e-4, 3e-4, 1e-3]      # Try different learning rates
  clip_ratios: [0.1, 0.2, 0.3]            # Try different clipping values  
  entropy_coefs: [0.0, 0.01, 0.05]        # Try different exploration levels
  network_sizes: 
    - [32, 32]                             # Smaller networks
    - [64, 64]                             # Standard size  
    - [128, 128]                           # Larger networks
  
  # Advanced experiments
  different_optimizers: ["adam", "sgd", "rmsprop"]
  batch_sizes: [512, 1024, 2048, 4096]
  ppo_epochs: [1, 2, 4, 8]

# Performance expectations
expected_performance:
  initial_reward: 20          # Random policy gets ~20 reward
  learning_threshold: 50      # Should exceed 50 within 10k steps
  solved_threshold: 195       # CartPole is "solved" at 195 average reward
  optimal_performance: 200    # Maximum possible reward is 200
  
  training_time:
    steps_to_learn: 5000      # Should start improving within 5k steps
    steps_to_solve: 25000     # Should solve within 25k steps  
    total_recommended: 50000   # Recommended total training steps

# Troubleshooting tips
troubleshooting:
  no_learning:
    - "Check if networks are updating (look at grad_norm in logs)"
    - "Verify action sampling is working correctly"
    - "Make sure advantages are being computed"
    
  unstable_learning:
    - "Try lower learning rate (1e-4)"
    - "Try smaller clip_ratio (0.1)" 
    - "Check for NaN values in losses"
    
  slow_learning:
    - "Try higher learning rate (1e-3)"
    - "Try higher entropy coefficient (0.05)"
    - "Check if value function is learning"