# PPO MiniGrid DoorKey-8x8 Experiment Configuration (From Scratch)
#
# This configuration trains PPO on the larger MiniGrid DoorKey-8x8 environment
# starting from scratch without any transfer learning.
# This is more challenging than 5x5 due to larger space and longer horizons.
# The goal is to achieve reasonable success rate through exploration and learning.
#
# TRAINING APPROACH:
# - Training from scratch with no pre-trained weights
# - Higher entropy for better exploration in larger space
# - Longer training duration to allow for learning
# - Adjusted hyperparameters for from-scratch learning

experiment:
  name: "ppo_minigrid_doorkey_8x8"
  seed: 42
  device: "cpu"
  debug: true

algorithm:
  name: "ppo"
  
  # Hyperparameters optimized for from-scratch learning in larger space
  lr: 3e-4
  clip_ratio: 0.2
  value_coef: 0.5
  entropy_coef: 0.12           # Higher entropy for exploration from scratch
  
  # Training schedule
  ppo_epochs: 4
  minibatch_size: 128
  max_grad_norm: 0.5
  
  # Advantage computation
  normalize_advantages: true
  clip_value_loss: true

environment:
  name: "MiniGrid-DoorKey-8x8-v0"
  wrapper: "minigrid"
  obs_mode: "image"            # Same observation mode as 5x5
  normalize_obs: false
  normalize_reward: false
  max_episode_steps: 100       # Longer episodes for larger environment
  render_mode: null

network:
  # Actor network - CNN architecture for from-scratch learning
  actor:
    type: "minigrid_cnn"
    activation: "relu"
    use_direction: true
    channels: [16, 32, 64]
    output_activation: "linear"
    
  # Critic network - same architecture  
  critic:
    type: "minigrid_cnn"
    activation: "relu"
    use_direction: true
    channels: [16, 32, 64]
    output_activation: "linear"

buffer:
  type: "trajectory"
  capacity: 12288             # Larger buffer for more complex environment
  batch_size: 12288
  
  # GAE parameters
  gamma: 0.99
  gae_lambda: 0.95
  compute_returns: true
  normalize_advantages: true

training:
  total_timesteps: 1000000    # More steps needed for from-scratch learning
  eval_frequency: 15000       # Evaluate every 15k steps
  checkpoint_frequency: 50000 # Save every 50k steps
  num_eval_episodes: 50
  
  # No transfer learning - starting from scratch
  # load_checkpoint: null     # Remove transfer learning
  # freeze_layers: []         # No layers to freeze
  
  # Success-based early stopping (adjusted for from-scratch)
  early_stopping: true
  target_success_rate: 0.50   # More realistic target from scratch
  patience: 6

logging:
  terminal: true
  tensorboard: true
  log_frequency: 3000
  
  custom_metrics:
    - "env/success_rate"
    - "env/episode_length"
    - "policy/entropy"
    - "policy/kl_divergence"   # Monitor policy changes during transfer
  
  wandb_enabled: true
  wandb:
    project: "rl-lab-minigrid"
    tags: ["ppo", "doorkey", "8x8", "from-scratch", "cnn"]
    notes: "PPO on DoorKey-8x8 training from scratch without transfer learning"

# Expected performance (from scratch learning)
expected_performance:
  random_success_rate: 0.005  # Random policy: ~0.5% success (much harder)
  initial_performance: 0.005  # Starting from random performance
  learning_threshold: 0.10    # Should exceed 10% within 200k steps  
  target_success_rate: 0.50   # Realistic target: 50% success by 1M steps
  
  episode_length:
    random_policy: 95         # Random policy hits timeout
    optimal_policy: 25        # Optimal path is ~25 steps  
    target_efficiency: 50     # Target average â‰¤50 steps (more lenient)

# Hyperparameter variants for from-scratch 8x8 tuning
experimentation:
  entropy_coefficients: [0.08, 0.12, 0.16]  # Higher exploration needed
  buffer_sizes: [8192, 12288, 16384]        # Larger buffers
  episode_limits: [100, 125, 150]           # Longer episodes for exploration
  learning_rates: [1e-4, 3e-4, 5e-4]       # From-scratch learning rates
  
  # Advanced techniques for from-scratch learning
  techniques:
    curiosity_driven: "Add intrinsic motivation for exploration"
    count_based_exploration: "Bonus for visiting rare states"
    longer_episodes: "Allow more time for exploration"

# Troubleshooting specific to from-scratch 8x8
troubleshooting:
  slow_initial_learning:
    - "Increase entropy_coef to 0.16 for more exploration"
    - "Extend max_episode_steps to 125 or 150"
    - "Consider adding exploration bonuses"
    
  exploration_insufficient:
    - "Add count-based exploration bonuses"
    - "Try curiosity-driven exploration"
    - "Increase buffer size for more diverse experiences"
    
  success_rate_too_low:
    - "Train for longer (1.5M or 2M timesteps)"
    - "Try curriculum learning starting from 5x5"
    - "Consider reward shaping or dense rewards"
    
  memory_issues:
    - "Reduce buffer_size if running out of memory" 
    - "Use gradient accumulation with smaller minibatch_size"
    - "Consider using MPS device on Apple Silicon Macs"