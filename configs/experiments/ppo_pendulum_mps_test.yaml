experiment:
  name: "ppo_pendulum_mps_test"
  seed: 42
  device: "mps"
  debug: false

algorithm:
  name: "ppo"
  
  # Separate learning rates for actor and critic
  actor_lr: 1e-4
  critic_lr: 3e-6  # Increased critic lr to 3e-6 for faster learning
  
  # Enable debug logging for network outputs
  debug_network_outputs: true
  
  # Conservative clipping for continuous control
  clip_ratio: 0.1
  
  # Slightly increased value coefficient for stronger critic training
  value_coef: 0.7
  
  # Higher entropy for continuous control exploration
  entropy_coef: 0.05
  
  # PPO training parameters
  ppo_epochs: 4
  minibatch_size: 64
  max_grad_norm: 0.5
  
  # CRITICAL FIX: Disable value loss clipping for learning from scratch
  clip_value_loss: false
  
  # Much tighter std bounds for precise control
  log_std_min: -2.3   # log(0.1) ≈ -2.3, so min std = 0.1
  log_std_max: -0.5   # log(0.6) ≈ -0.5, so max std = 0.6
  
  # Continuous control parameters
  log_std_init: -1.5  # Start with std=exp(-1.5)≈0.22 for precise exploration
  action_bounds: [[-2.0, 2.0]]  # Pendulum torque bounds: [low, high] for 1D action
  use_tanh_squashing: true
  
  # Advantage computation
  normalize_advantages: true
  clip_value_loss: true

environment:
  name: "Pendulum-v1"
  wrapper: "gym"
  normalize_obs: false
  normalize_reward: false

network:
  # Use continuous_actor_mlp for proper continuous control
  actor:
    type: "continuous_actor_mlp"
    hidden_dims: [128, 128, 64, 64]  # Deeper network with 4 layers
    activation: "tanh"
    
  critic:
    type: "critic_mlp"
    hidden_dims: [128, 128]  # Simpler 2-layer network for more stable training
    activation: "tanh"
    output_activation: "linear"

buffer:
  type: "trajectory"
  # Smaller buffer for dense reward environment
  capacity: 2048
  batch_size: 2048
  
  # GAE parameters - standard for dense rewards
  gamma: 0.99
  gae_lambda: 0.95
  compute_returns: true
  normalize_advantages: true

training:
  # Short training for testing MPS
  total_timesteps: 10000
  eval_frequency: 5000
  checkpoint_frequency: 10000
  num_eval_episodes: 5
  
  # Disable early stopping for full training
  early_stopping: false
  target_reward: -200  # Good performance for pendulum (random gets ~-1000)
  patience: 2

logging:
  terminal: true
  tensorboard: true
  log_frequency: 1000
  
  wandb_enabled: false