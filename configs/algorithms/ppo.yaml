# PPO Algorithm Configuration
# 
# This file contains the hyperparameters for Proximal Policy Optimization (PPO).
# These are well-tuned defaults that work across many environments.
#
# Key hyperparameters to understand:
# - lr: Learning rate for both actor and critic networks
# - clip_ratio: PPO clipping parameter (typically 0.1-0.3)
# - value_coef: Weight for value function loss in total loss
# - entropy_coef: Weight for entropy bonus (encourages exploration)
# - ppo_epochs: Number of optimization epochs per batch of data
# - minibatch_size: Size of minibatches for gradient updates
#
# HOMEWORK NOTE: These values are good defaults, but you might want to
# experiment with them to see how they affect training performance.

# Learning parameters
lr: 3e-4                    # Learning rate (Adam optimizer)
clip_ratio: 0.2             # PPO clipping parameter 
value_coef: 0.5             # Coefficient for value function loss
entropy_coef: 0.01          # Coefficient for entropy bonus

# Training parameters  
ppo_epochs: 4               # Number of epochs to train on each batch
minibatch_size: 64          # Size of minibatches for training
max_grad_norm: 0.5          # Maximum gradient norm for clipping

# Normalization and clipping
normalize_advantages: true   # Whether to normalize advantages
clip_value_loss: true       # Whether to clip value function loss

# Network architecture (these will be combined with network config)
# The actual network architecture is specified in the network section
# of the experiment config, but these are PPO-specific network settings
network:
  shared_backbone: false    # Whether actor and critic share a backbone
  initialization: "orthogonal"  # Weight initialization scheme
  
# Advanced PPO options (for experienced users)
advanced:
  # Adaptive KL penalty (alternative to clipping)
  use_kl_penalty: false
  target_kl: 0.01
  kl_coef: 0.2
  
  # Early stopping based on KL divergence
  early_stopping: false
  kl_threshold: 0.015
  
  # Value function options
  value_loss_type: "mse"    # "mse" or "huber"
  huber_delta: 1.0          # Delta for Huber loss (if used)

# Environment-specific tweaks
# These can override the defaults above for specific environments
environment_overrides:
  # For continuous control environments
  continuous:
    entropy_coef: 0.0       # Often lower entropy for continuous control
    
  # For discrete action environments  
  discrete:
    entropy_coef: 0.01      # Standard entropy for discrete actions
    
  # For high-dimensional state spaces
  high_dim:
    lr: 1e-4                # Often need lower learning rate
    clip_ratio: 0.1         # Smaller clipping for stability